{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Preprocessing Cyber Threat Intelligence Dataset for Llama-3 Model\n",
    "\n",
    "This notebook demonstrates the process of loading, preprocessing, and preparing the *Cyber Threat Intelligence (CTI)* dataset from Hugging Face for training a decoder-only model. The dataset is used to extract entities, their relationships, and generate a diagnosis from threat reports.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "- **Dataset Name**: \"swaption2009/cyber-threat-intelligence-custom-data\"\n",
    "- **Content**: The dataset consists of cybersecurity threat reports. Each report contains detailed information about a cyber attack or threat, including entities (e.g., attack types, actors, systems), their relationships (e.g., attack vector, methods), and the diagnosis of the threat. The goal is to train the model to process these reports, extract meaningful entities and relations, and generate a comprehensive diagnosis.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "1. **Data Splitting**\n",
    "   - The dataset is split into three sets:\n",
    "     - **80% for training**\n",
    "     - **10% for validation**\n",
    "     - **10% for testing**\n",
    "   - This split ensures the model is trained on a large portion of the data while maintaining sufficient data for evaluation and tuning.\n",
    "\n",
    "2. **Data Preprocessing (with custom library Pipeline)**\n",
    "   - The input prompt given to the model is constructed as follows:\n",
    "\n",
    "     ```\n",
    "     You are a skilled AI Agent capable of doing CTI Analysis.\n",
    "     Given this threat report:\n",
    "     \n",
    "     {threat report}\n",
    "     \n",
    "     You will extract the main entities and their relations; finally, you will generate a diagnosis of the threat.\n",
    "     ```\n",
    "\n",
    "   - The expected output is formatted to extract the entities, relationships, and the diagnosis:\n",
    "     \n",
    "     ```\n",
    "     Entities: {list of entities}\n",
    "     Relations: {list of relations} \n",
    "     Diagnosis: {diagnosis}\n",
    "     ```\n",
    "     \n",
    "   - Since a decoder-only model is used, the expected output is integrated directly into the input prompt.\n",
    "   - The output sequence also contains the input prompt, but it is masked using the special token `-100`, ensuring that the loss function ignores it during training. This allows the model to focus on predicting the expected output rather than memorizing the input prompt.\n",
    "\n",
    "3. **Tokenization and Dataset Packing**\n",
    "   - Both the input prompt and expected output are tokenized and packed into a dataset using the Hugging Face dataset format.\n",
    "   - This format ensures that the data is ready for training using Hugging Face's `Trainer` API or any other model training utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from Pipeline.data_retrieving.HuggingFace_DataRetriever import *\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"swaption2009/cyber-threat-intelligence-custom-data\"\n",
    "\n",
    "data_retriever = HuggingFace_DataRetriever(huggingface_dataset_name)\n",
    "dataset = data_retriever.retrieve_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'diagnosis', 'solutions'],\n",
       "        num_rows: 476\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the dataset is already split, but the only split available is the training set. We will then use the data available to create our new splits:\n",
    "- 80% training\n",
    "- 10% validation\n",
    "- 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'diagnosis', 'solutions'],\n",
       "        num_rows: 380\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'diagnosis', 'solutions'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'diagnosis', 'solutions'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% of the data will be used for training, the remaining 20% will be further split into 50% for validation and 50% for testing\n",
    "first_split = dataset.train_test_split(train_size=0.8,test_size=0.2,shuffle=False)\n",
    "\n",
    "# split the remaining 20% into 50% for validation and 50% for testing\n",
    "second_split = first_split['test'].train_test_split(train_size=0.5,test_size=0.5,shuffle=False)\n",
    "\n",
    "# create the new dataset\n",
    "dataset = DatasetDict({\n",
    "    'train': first_split['train'],\n",
    "    'validation': second_split['train'],\n",
    "    'test': second_split['test']\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline.preprocessing.CTI_Preprocessor import *\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"YOUR HUGGING FACE ACCESS TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the necessary preprocessing parameters and create an instance of the preprocessor\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_preprocessing_params={\n",
    "        'tokenizer': tokenizer,\n",
    "        'truncation': True\n",
    "    }\n",
    "\n",
    "output_preprocessing_params={\n",
    "        'tokenizer': tokenizer,\n",
    "        'truncation': True\n",
    "    }\n",
    "\n",
    "preprocessor = CTI_Preprocessor(\n",
    "    input_preprocessing_params,\n",
    "    output_preprocessing_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the training set\n",
    "tokenized_training_inputs = preprocessor.preprocess_input_data(dataset['train'].to_pandas())\n",
    "tokenized_training_outputs = preprocessor.preprocess_output_data(dataset['train'].to_pandas())\n",
    "\n",
    "# preprocess the validation set\n",
    "tokenized_validation_inputs = preprocessor.preprocess_input_data(dataset['validation'].to_pandas())\n",
    "tokenized_validation_outputs = preprocessor.preprocess_output_data(dataset['validation'].to_pandas())\n",
    "\n",
    "# preprocess the test set\n",
    "tokenized_test_inputs = preprocessor.preprocess_input_data(dataset['test'].to_pandas(), for_training=False)\n",
    "tokenized_test_outputs = preprocessor.preprocess_output_data(dataset['test'].to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's show an example of the traing data that will be fed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promt example:\n",
      "\n",
      "\n",
      "You are a skilled AI Agent capable of doing CTI Analysis.\n",
      "\n",
      "Given this threat report:\n",
      "A cybersquatting domain save-russia[.]today is launching DoS attacks on Ukrainian news sites.\n",
      "\n",
      "You will extract the main entities and their relations; finally, you will generate a diagnosis of the threat.\n",
      "\n",
      "Entities: cybersquatting (attack-pattern), save-russia[.]today (url), DoS attacks (attack-pattern), Ukrainian news sites (identity)\n",
      "Relations: DoS attacks to Ukrainian news sites (targets), cybersquatting to save-russia[.]today (uses), DoS attacks to save-russia[.]today (uses)\n",
      "Diagnosis: The diagnosis is a cyber attack that involves the use of a cybersquatting domain save-russia[.]today to launch DoS attacks on Ukrainian news sites. The attacker targets the Ukrainian news sites as the victim, using the cybersquatting\n",
      "\n",
      "\n",
      "\n",
      "Expected response of the model:\n",
      "\n",
      "\n",
      "\n",
      "Entities: cybersquatting (attack-pattern), save-russia[.]today (url), DoS attacks (attack-pattern), Ukrainian news sites (identity)\n",
      "Relations: DoS attacks to Ukrainian news sites (targets), cybersquatting to save-russia[.]today (uses), DoS attacks to save-russia[.]today (uses)\n",
      "Diagnosis: The diagnosis is a cyber attack that involves the use of a cybersquatting domain save-russia[.]today to launch DoS attacks on Ukrainian news sites. The attacker targets the Ukrainian news sites as the victim, using the cybersquatting\n"
     ]
    }
   ],
   "source": [
    "print('Promt example:\\n\\n')\n",
    "print(tokenizer.decode(tokenized_training_inputs[0], skip_special_tokens=True))\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Expected response of the model:\\n\\n')\n",
    "output = tokens = [token for token in tokenized_training_outputs[0] if token != -100] # remove the -100 tokens (padding), which would generate an error when using the decode() method\n",
    "print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the preparation of the prompt and the output is correct, the lenghts of the tokenized prompt and the lenght of the (masked) output should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of the first example\n",
    "len(tokenized_training_inputs[0]) == len(tokenized_training_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print an example of the testing prompt and its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promt example:\n",
      "\n",
      "\n",
      "You are a skilled AI Agent capable of doing CTI Analysis.\n",
      "\n",
      "Given this threat report:\n",
      "The PDF  exploits CVE-2013-2729 to download a binary which also installed CryptoWall 2.0.\n",
      "\n",
      "You will extract the main entities and their relations; finally, you will generate a diagnosis of the threat.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Expected response of the model:\n",
      "\n",
      "\n",
      "\n",
      "Entities: CryptoWall 2.0 (malware), CVE-2013-2729 (vulnerability)\n",
      "Relations: CryptoWall 2.0 to CVE-2013-2729 (exploits)\n",
      "Diagnosis: The entity is a PDF file and the cybersecurity issue is a vulnerability (CVE-2013-2729) that is being exploited. The PDF file is being used as a delivery mechanism for malware (CryptoWall 2.0). The diagnosis is\n"
     ]
    }
   ],
   "source": [
    "print('Promt example:\\n\\n')\n",
    "print(tokenizer.decode(tokenized_test_inputs[0], skip_special_tokens=True))\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print('Expected response of the model:\\n\\n')\n",
    "output = tokens = [token for token in tokenized_test_outputs[0] if token != -100] # remove the -100 tokens (padding), which would generate an error when using the decode() method\n",
    "print(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save locally the dataset, using the Hugging Face Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create a dictionay\n",
    "dataset = {\n",
    "    'train_set': {\n",
    "                    'input_ids': tokenized_training_inputs,\n",
    "                    'labels': tokenized_training_outputs        \n",
    "                },\n",
    "\n",
    "    'validation_set': {\n",
    "                    'input_ids': tokenized_validation_inputs,\n",
    "                    'labels': tokenized_validation_outputs        \n",
    "                },\n",
    "    \n",
    "    'test_set': {\n",
    "                    'input_ids': tokenized_test_inputs,\n",
    "                    'labels': tokenized_test_outputs        \n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then convert each split into a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(dataset[\"train_set\"])\n",
    "validation_dataset = Dataset.from_dict(dataset[\"validation_set\"])\n",
    "test_dataset = Dataset.from_dict(dataset[\"test_set\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally pack it into a DatasetDict structure\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "import pickle\n",
    "\n",
    "# Open a file in write-binary mode\n",
    "with open(\"data/dataset_CTI_llama3_2-3B.pkl\", \"wb\") as file:\n",
    "    # Serialize the list and save it to the file\n",
    "    pickle.dump(dataset, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
